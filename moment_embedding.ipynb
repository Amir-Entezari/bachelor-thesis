{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_file = 'dataset/participants.tsv'\n",
    "participants_df = pd.read_csv(participants_file, sep='\\t')\n",
    "\n",
    "subject_ids = participants_df['participant_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class EEGDataLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, dir, list_IDs, standardize=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dir (str): Directory where the processed EEG chunks are stored.\n",
    "            list_IDs (list): List of subject paths relative to the base directory.\n",
    "            sampling_rate (int): Target sampling rate for resampling the data.\n",
    "        \"\"\"\n",
    "        self.dir = dir\n",
    "        self.list_IDs = list_IDs  # List of subject directories\n",
    "        self.standardize = standardize\n",
    "        self.label_map = {\n",
    "            \"A\": 0,\n",
    "            \"C\": 1,\n",
    "            \"F\": 2\n",
    "        }\n",
    "        # Gather all chunk file paths from the subject directories\n",
    "        self.chunk_paths = []\n",
    "        for subject_id in list_IDs:\n",
    "            subject_dir = os.path.join(dir, subject_id)\n",
    "            self.chunk_paths.extend(\n",
    "                [\n",
    "                    os.path.join(subject_id, file)\n",
    "                    for file in os.listdir(subject_dir)\n",
    "                    if file.endswith(\".pt\")\n",
    "                ]\n",
    "            )\n",
    "        self.chunk_paths.sort()\n",
    "        # If we are standardizing, initialize a StandardScaler\n",
    "        if self.standardize:\n",
    "            self.scaler = StandardScaler()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of chunks available.\"\"\"\n",
    "        return len(self.chunk_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx (int): Index to retrieve a chunk and its label.\n",
    "\n",
    "        Returns:\n",
    "            X (torch.Tensor): EEG data as a tensor.\n",
    "            y (int): Label corresponding to the chunk.\n",
    "        \"\"\"\n",
    "        # Load the .pt file for the given chunk\n",
    "        chunk_path = os.path.join(self.dir, self.chunk_paths[idx])\n",
    "        sample = torch.load(chunk_path)\n",
    "\n",
    "        # Extract data and labels\n",
    "        eeg_data = sample[\"data\"]\n",
    "        label = sample[\"label\"]\n",
    "        # Resample if necessary\n",
    "        # eegsample = resample(eeg_data, int(eeg_data.shape[-1] * self.sampling_rate / self.default_rates), axis=-1)\n",
    "\n",
    "\n",
    "        # Apply standard scaling to each channel independently\n",
    "        if self.standardize:\n",
    "            # Standardize each channel independently (along the time points)\n",
    "            eeg_data = self.scaler.fit_transform(eeg_data.T).T\n",
    "\n",
    "        # # Normalize the data\n",
    "        # eegsample = eegsample / (\n",
    "        #     np.quantile(\n",
    "        #         np.abs(eegsample), q=0.95, interpolation=\"linear\", axis=-1, keepdims=True\n",
    "        #     )\n",
    "        #     + 1e-8\n",
    "        # )\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        eegsample = torch.from_numpy(eeg_data).float()\n",
    "\n",
    "        # Extract the target label (e.g., Group)\n",
    "        X = eegsample\n",
    "        y = self.label_map[label]  # Adjust this if the target label changes\n",
    "        return X, y\n",
    "\n",
    "# Example usage\n",
    "# Path to dataset and labels\n",
    "data_dir = \"dataset/derivatives/processed_dataset\"\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = EEGDataLoader(data_dir, subject_ids)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Iterate through the DataLoader\n",
    "# for batch in dataloader:\n",
    "#     print(f\"Batch Shape: {batch[0].shape}\")\n",
    "#     break\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    samples, labels = [], []\n",
    "\n",
    "    for i, l in batch:\n",
    "\n",
    "        samples.append(i)\n",
    "\n",
    "        labels.append(l)\n",
    "\n",
    "    samples = torch.stack(samples, dim = 0)\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    batch = samples\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(f\"Batch Shape: {batch[1]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = sample = torch.load('dataset/derivatives/processed_dataset/sub-001/sub-001_task-eyesclosed_eeg_chunk_037.pt')\n",
    "raw['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from momentfm import MOMENTPipeline\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size, n_classes):\n",
    "        super().__init__()\n",
    "        self.clshead = nn.Sequential(\n",
    "            nn.ELU(),\n",
    "            nn.Linear(emb_size, n_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.clshead(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: `embeddings`, shape (batch, max_len, d_model)\n",
    "        Returns:\n",
    "            `encoder input`, shape (batch, max_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "def create_patch(xb, patch_len, stride, max_sequence_length):\n",
    "    \"\"\"\n",
    "    xb: [bs x n_vars x seq_len ]\n",
    "    \"\"\"\n",
    "\n",
    "    seq_len = max_sequence_length if max_sequence_length is not None else xb.shape[2]\n",
    "\n",
    "    mask = torch.ones(xb.shape)\n",
    "\n",
    "    num_patch = math.ceil((max(seq_len, patch_len) - patch_len) / stride) + 1\n",
    "\n",
    "    tgt_len = patch_len + stride * (num_patch -1)\n",
    "\n",
    "    pd = tgt_len - seq_len\n",
    "\n",
    "    pad1 = (0, pd)\n",
    "\n",
    "    xb = F.pad(xb, pad1, \"constant\", 0)\n",
    "\n",
    "    mask = F.pad(mask, pad1, \"constant\", 0)\n",
    "\n",
    "    xb = xb.unfold(dimension=-1, size=patch_len, step=stride)                 # xb: [bs x n_vars x num_patch x patch_len]\n",
    "\n",
    "    mask = mask.unfold(dimension=-1, size=patch_len, step=stride)                 # xb: [bs x n_vars x num_patch x patch_len]\n",
    "\n",
    "    return xb, mask\n",
    "\n",
    "    \n",
    "\n",
    "class MomentEEG(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_size=512,\n",
    "        n_channels=16,\n",
    "        patch_len=512,\n",
    "        stride=512,\n",
    "        max_sequence_length=2560,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = MOMENTPipeline.from_pretrained(\n",
    "            \"AutonLab/MOMENT-1-small\",\n",
    "            model_kwargs={'task_name': 'embedding', 'reduction': 'mean'},\n",
    "        )\n",
    "        self.tokenizer.init()\n",
    "\n",
    "        self.tokenizer.eval()\n",
    "\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "\n",
    "    def forward(self, x, perturb=False):\n",
    "        \"\"\"\n",
    "        x: [batch_size, channel, num_patch, ts]\n",
    "        output: [batch_size, emb_size]\n",
    "        \"\"\"\n",
    "        x, m = create_patch(x, patch_len=self.patch_len, stride=self.stride, max_sequence_length=self.max_sequence_length)\n",
    "        emb_seq = []\n",
    "\n",
    "        for i in range(x.shape[1]):\n",
    "            xb = x[:, i: i + 1, :, :].squeeze(1)\n",
    "            mb = m[:, i: i + 1, :, :].squeeze(1)\n",
    "            bs, num_patch, patch_len = xb.shape\n",
    "            xb = torch.reshape(xb, (bs * num_patch, 1, patch_len))\n",
    "            mb = torch.reshape(mb, (bs * num_patch, 1, patch_len))\n",
    "\n",
    "            with torch.no_grad():\n",
    "                xb = self.tokenizer(x_enc=xb.detach(), input_mask=mb.squeeze(1).detach(), mask=mb.detach()).embeddings\n",
    "            # print(xb.shape)\n",
    "            xb = torch.reshape(xb, (bs, num_patch, -1))\n",
    "\n",
    "            emb_seq.append(xb)\n",
    "\n",
    "        # Return the raw embeddings\n",
    "        emb = torch.cat(emb_seq, dim=1)  # (batch_size, 16 * ts, emb)\n",
    "        # emb = emb.mean(dim=1)  # (batch_size, emb)\n",
    "        # print(emb.shape)\n",
    "        return emb  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define root directories\n",
    "data_dir = \"dataset/derivatives/processed_dataset\"\n",
    "labels_file = \"dataset/participants.tsv\"\n",
    "\n",
    "all_data_loader = DataLoader(\n",
    "        EEGDataLoader(data_dir, subject_ids),\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=2,\n",
    "        persistent_workers=True,\n",
    "        collate_fn = collate_fn\n",
    "    )\n",
    "\n",
    "print(len(all_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'dataset/derivatives/embeddings'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "model = MomentEEG(512, n_channels=19, max_sequence_length = 3000).to(device)\n",
    "\n",
    "i = 0\n",
    "for X, y in all_data_loader:\n",
    "    batch_embedd_eeg=model(X.to(device))\n",
    "    sample_emb = {\n",
    "                \"data\": batch_embedd_eeg,\n",
    "                \"label\": y\n",
    "            }\n",
    "    output_file_name = all_data_loader.dataset.chunk_paths[i]\n",
    "    subject_id = output_file_name.split('/')[0]\n",
    "    output_file_path = os.path.join(output_dir, output_file_name)\n",
    "    if not os.path.exists(os.path.join(output_dir, subject_id)):\n",
    "        os.makedirs(os.path.join(output_dir, subject_id))\n",
    "    print(output_file_path)\n",
    "    print(batch_embedd_eeg.shape)\n",
    "    torch.save(sample_emb, output_file_path)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = sample = torch.load('dataset/derivatives/embeddings/sub-001/sub-001_task-eyesclosed_eeg_chunk_000.pt')\n",
    "raw['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentClassifier(nn.Module):\n",
    "    def __init__(self, emb_size=512, n_channels=16, n_classes=6, **kwargs):\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "        # Channel embedding and positional encoding will now be handled here\n",
    "        self.positional_encoding = PositionalEncoding(emb_size)\n",
    "\n",
    "        # Initialize learnable channel embeddings\n",
    "        self.channel_tokens = nn.Embedding(n_channels, emb_size)\n",
    "        self.index = nn.Parameter(torch.LongTensor(range(n_channels)), requires_grad=False)\n",
    "\n",
    "        # If you want to add a classification head, you can do that here\n",
    "        self.classifier = nn.Linear(emb_size, n_classes)\n",
    "\n",
    "    def forward(self, x, perturb=False, saved_embeddings=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, channel, num_patch, ts]\n",
    "        saved_embeddings: Pre-computed embeddings that bypass the `MomentEEG` forward pass.\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply channel embedding and positional encoding here\n",
    "        batch_size, ts, _ = x.shape\n",
    "        channel_emb = []\n",
    "\n",
    "        for i in range(self.n_channels):\n",
    "            # Channel token embedding (repeat across time steps)\n",
    "            channel_token_emb = (\n",
    "                self.channel_tokens(self.index[i])\n",
    "                .unsqueeze(0)\n",
    "                .unsqueeze(0)\n",
    "                .repeat(batch_size, ts, 1)\n",
    "            )\n",
    "\n",
    "            # Add positional encoding to the embeddings\n",
    "            emb_with_channel_pos = self.positional_encoding(x + channel_token_emb)\n",
    "            channel_emb.append(emb_with_channel_pos)\n",
    "\n",
    "        # Stack embeddings from all channels and average them\n",
    "        emb = torch.cat(channel_emb, dim=1)  # (batch_size, 16 * ts, emb)\n",
    "\n",
    "\n",
    "        # (batch_size, emb)\n",
    "        emb = emb.mean(dim=1)\n",
    "        \n",
    "        # Optionally, add a classification head here (not added in this code snippet)\n",
    "        emb = self.classifier(emb)\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MomentClassifier(n_channels=19, n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(raw['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "participants_file = 'dataset/participants.tsv'\n",
    "participants_df = pd.read_csv(participants_file, sep='\\t')\n",
    "\n",
    "subject_ids = participants_df['participant_id'].tolist()\n",
    "\n",
    "# Perform LOSO cross-validation splits\n",
    "loso_splits = []\n",
    "for test_subject in subject_ids:\n",
    "    # Test set is the current subject\n",
    "    test_set = [test_subject]\n",
    "\n",
    "    # Randomly select 6 subjects for validation\n",
    "    validation_subjects = random.sample(subject_ids, 6)\n",
    "    while test_subject in validation_subjects:\n",
    "        validation_subjects = random.sample(subject_ids, 6)\n",
    "        \n",
    "    train_set = []\n",
    "    # Training set is all other subjects\n",
    "    for subject in subject_ids:\n",
    "        if subject != test_subject and subject not in validation_subjects:# and int(subject[-3:]) <=3:\n",
    "            train_set.append(subject)\n",
    "    # train_set = [subject for subject in subject_ids if subject != test_subject]\n",
    "\n",
    "    # Append the split to the list\n",
    "    loso_splits.append({'train': train_set, \n",
    "                        'val': validation_subjects,\n",
    "                        'test': test_set})\n",
    "# for i, split in enumerate(loso_splits):\n",
    "#     print(f\"Fold {i + 1}:\")\n",
    "#     print(f\"  Train: {split['train']}\")\n",
    "#     print(f\"  Test: {split['test']}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed = 2024\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Define root directories\n",
    "data_dir = \"dataset/derivatives/embeddings\"\n",
    "labels_file = \"dataset/participants.tsv\"\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        EEGDataLoader(data_dir, loso_splits[0]['train']),\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=2,\n",
    "        persistent_workers=True,\n",
    "        collate_fn = collate_fn\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "        EEGDataLoader(data_dir, loso_splits[0]['val']),\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=2,\n",
    "        persistent_workers=True,\n",
    "        collate_fn = collate_fn\n",
    "    )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    EEGDataLoader(data_dir, loso_splits[0]['test']),\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    collate_fn = collate_fn\n",
    ")\n",
    "\n",
    "print(len(train_loader))\n",
    "\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "model_name = \"Moment\"\n",
    "if model_name == \"Moment\":\n",
    "    model = MomentClassifier(512, n_classes = 3,n_channels=19)\n",
    "else:\n",
    "    raise NotImiplementedError\n",
    "lightning_model = LitModel_finetune(model,args={\n",
    "        \"lr\":0.01,\n",
    "        \"weight_decay\":1e-5,\n",
    "        \"gamma\":0.1,\n",
    "        \"n_steps\":100,\n",
    "    })\n",
    "\n",
    "# logger and callbacks\n",
    "version = \"moment\"\n",
    "logfolder = \"log\"\n",
    "logger = TensorBoardLogger(\n",
    "    save_dir=\"./\",\n",
    "    version=version,\n",
    "    name=logfolder,\n",
    ")\n",
    "# early_stop_callback = EarlyStopping(\n",
    "#     monitor=\"val_f1\", patience= args.patience, verbose=False, mode=\"max\"\n",
    "# )\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k = 0,\n",
    "                                      monitor = \"epoch\",\n",
    "                                      mode = \"max\",\n",
    "                                      save_last = True\n",
    "                                        )\n",
    "\n",
    "tqdm_progress_bar = TQDMProgressBar(refresh_rate= 20, process_position=0)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    # devices=1,  # Set devices to an integer instead of a list when using CPU\n",
    "    devices=[0],  # Use list format only when specifying GPUs\n",
    "    accelerator=\"auto\",\n",
    "    strategy='ddp_notebook',\n",
    "    benchmark=True,\n",
    "    enable_checkpointing=True,\n",
    "    logger=logger,\n",
    "    max_epochs=10,\n",
    "    callbacks= [checkpoint_callback, tqdm_progress_bar], # [early_stop_callback, tqdm_progress_bar],\n",
    "    log_every_n_steps = 1,\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer.fit(\n",
    "    lightning_model, train_dataloaders=train_loader, val_dataloaders=val_loader\n",
    ")\n",
    "\n",
    "# test the model\n",
    "pretrain_result = trainer.test(\n",
    "    model=lightning_model, ckpt_path=\"last\", dataloaders=test_loader\n",
    ")[0]\n",
    "print(pretrain_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
